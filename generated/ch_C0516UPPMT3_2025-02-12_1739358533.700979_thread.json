{
  "id": "ch_C0516UPPMT3_2025-02-12_1739358533.700979_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "vineeth",
    "Nathaniel",
    "jordy.vlan",
    "Hamish"
  ],
  "messages": [
    {
      "sender": "Hamish",
      "user_id": "U06G3LGDBQU",
      "message": "@vineeth any thoughts?",
      "time": "03:08",
      "timestamp": "1739358533.700979",
      "is_reply": false
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "To be fair, we dont have a strong defence against prompt injection, we leave that upto the LLM providers :neutral_face:",
      "time": "03:55",
      "timestamp": "1739361348.285039",
      "is_reply": true
    },
    {
      "sender": "Hamish",
      "user_id": "U06G3LGDBQU",
      "message": "Can someone help me understand what the LLM providers e.g. Open AI, do on their side so I can formulate a response?",
      "time": "04:39",
      "timestamp": "1739363958.816019",
      "is_reply": true
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "They say they have good system prompts to prevent prompt injection and jail breaks like the infamous DINO",
      "time": "04:44",
      "timestamp": "1739364250.327589",
      "is_reply": true
    },
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "for Claude the system prompts designed to defend against prompt injection are publicly available here: https://docs.anthropic.com/en/release-notes/system-prompts#july-12th-2024",
      "time": "04:46",
      "timestamp": "1739364388.047419",
      "is_reply": true
    },
    {
      "sender": "Nathaniel",
      "user_id": "U04BGHM4AEL",
      "message": "Would it be fair to say that we're also at a lower risk of prompt injection since the document's content, even if they were able to slip a prompt in unnoticed, is passed to the model as context to extract data from, not as a prompt itself?\n\nLike their prompt injection attack would need to override not only the model's system prompt, but also the IB system prompt that is included with whichever type of Build field it is.  And the document data is surely wrapped in some sort of indicators to flag to the model where the prompting ends and the document context begins, right?",
      "time": "09:09",
      "timestamp": "1739380179.888199",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2025-02-12.json",
    "message_count": 6,
    "start_time": "1739358533.700979",
    "end_time": "1739380179.888199",
    "is_thread": true
  }
}