{
  "id": "ch_C06FA6A23_2021-12-07_1638866239.277300_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Nidhi",
    "sudeep"
  ],
  "messages": [
    {
      "sender": "Nidhi",
      "user_id": "U0179LPQK29",
      "message": "Were the flows used for benchmarking using model service?\n@achao is working on benchmarking model service which should give us better resource/latency numbers. Unfortunately, every model has a different memory/latency profile so itâ€™s hard to avoid some variability here.",
      "time": "00:37",
      "timestamp": "1638866239.277300",
      "is_reply": false
    },
    {
      "sender": "sudeep",
      "user_id": "UCWRN72EP",
      "message": "Catching up here I think there are a few things that we should look at -\n1. There is a need to do an apples to apples comparison of the flow throughput before and after. I think we know that the platform will become more expensive over time - ex: how do flows that do not use models compare before and after for similar resources allocated to them.  \n2. As part of this we need to define better units of work for our system to be able to benchmark [think http throughput, RPC rates, i/o throughput, db throughput, ml training units] - currently a proxy is pages/hr but these are not standard and we cannot normalize the throughput for different inputs, underlying infrastructure types. \n3. Need to add more monitoring into our systems - we have been making progress here - but there is a need to get lot more done for each application to understand throughput and create benchmarks. I understand the desire for us to create individual benchmarking tests for services/usecases - however it's important to make those efforts repeatable by converting these into stats that our applications produce. \nThis will also play a significant role in resiliency testing of the platform - ex: as we move on from load tests to systematically introduce failures in the system and measure throughput/resiliency - these measurements will guide our investments on making things better. Happy to discuss this in more detail and chat about what we can do next here.",
      "time": "02:02",
      "timestamp": "1638871376.277900",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2021-12-07.json",
    "message_count": 2,
    "start_time": "1638866239.277300",
    "end_time": "1638871376.277900",
    "is_thread": true
  }
}