{
  "id": "ch_C0516UPPMT3_2025-01-21_1737457489.808359_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Ben Hope",
    "jordy.vlan",
    "lydia"
  ],
  "messages": [
    {
      "sender": "Ben Hope",
      "user_id": "U03GTMX8998",
      "message": "Folks,\n\nWith Ground Truth measurements involving LLMs, do we currently do / have we considered doing multiple runs and taking probabilities/percentiles of outcomes instead of just a 1-off “right/wrong”, similar to other benchmarking frameworks?\n\nProblem I am thinking of is where for some fields the LLM may return different results in different runs and how that affects measuring accuracy. I know in an ideal world there would instead be more consistency and the messaging gets difficult if we’re embracing inconsistency of outcomes, but this would provide a more reliable representation of solution performance",
      "time": "03:04",
      "timestamp": "1737457489.808359",
      "is_reply": false
    },
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "1/|Y| \\sum_{y \\in Y} LLMJudge(y, \\hat{y}) would already give more insight than binary and a single LLMJudge :wink: It is what we report in our ibllm benchmarks.",
      "time": "04:40",
      "timestamp": "1737463225.590659",
      "is_reply": true
    },
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "Also, even our current LLMJudge approach is rather vanilla, we should look into remediating some of the inherent biases: https://www.adaptive-ml.com/post/a-fair-fight",
      "time": "08:17",
      "timestamp": "1737476225.276369",
      "is_reply": true
    },
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "and as a bonus: how statisticians look at proper evaluation with LLMs: https://arxiv.org/pdf/2411.00640",
      "time": "10:39",
      "timestamp": "1737484768.800839",
      "is_reply": true
    },
    {
      "sender": "lydia",
      "user_id": "UJK4LKYSJ",
      "message": "That's a good question Ben! We have not aligned on an approach for this in the solution accuracy feature. For internal benchmarking, we generally rely on averaging across a few runs (I believe usually 3). cc @hannah @Serena for something for us to consider in the future for the solution accuracy feature.",
      "time": "12:06",
      "timestamp": "1737489978.691229",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2025-01-21.json",
    "message_count": 5,
    "start_time": "1737457489.808359",
    "end_time": "1737489978.691229",
    "is_thread": true
  }
}