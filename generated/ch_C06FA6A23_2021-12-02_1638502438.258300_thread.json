{
  "id": "ch_C06FA6A23_2021-12-02_1638502438.258300_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "kunal",
    "advait",
    "Shubham Oli"
  ],
  "messages": [
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "Hi Team,\n\ndo we know who is using `KinD` for their CI testing? I can see `kind` network in our build machines?",
      "time": "19:33",
      "timestamp": "1638502438.258300",
      "is_reply": false
    },
    {
      "sender": "kunal",
      "user_id": "U019YB70B8U",
      "message": "yeah — that is us. cc @advait\n\nIs it causing some issue?",
      "time": "19:37",
      "timestamp": "1638502625.258400",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "https://github.com/instabase/instabase/pull/22896 is our currently open PR on this - the Jenkins image with Kind we pushed out is `21.11.16.16.58.23`",
      "time": "19:57",
      "timestamp": "1638503836.258700",
      "is_reply": true
    },
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "Got it.\n\nJust checking - are we deleting the kind cluster once tests are finished?",
      "time": "19:58",
      "timestamp": "1638503914.259000",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "let me know if there's issues with cleaning up the cluster - from my understanding it should be cleaning it up at the end of the integration test runs but can take a look if there's issues there",
      "time": "19:58",
      "timestamp": "1638503918.259200",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "ah yep, jinx",
      "time": "19:58",
      "timestamp": "1638503923.259400",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "yep - using `kind delete cluster` which should be deleting the cluster",
      "time": "19:59",
      "timestamp": "1638503997.259600",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "https://github.com/instabase/instabase/pull/22896/files#diff-a00b35e0e27664fc5ec4677e2e8feeef311eafbcae851bf57aa1086ab17d71c2R106",
      "time": "20:00",
      "timestamp": "1638504021.259800",
      "is_reply": true
    },
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "sure will let you know.\n\nI was doing some cleanup so I saw the kind being used.",
      "time": "20:00",
      "timestamp": "1638504037.260000",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "gotcha, thanks - from a while back there may be certain build machines where it wasn't cleaned up properly (from when I was trying to build out the pipeline) but it should be deleting consistently now",
      "time": "20:02",
      "timestamp": "1638504131.260300",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "taking another look at this, I think there may be a chance the cluster isn't deleted if the sequence of make commands crashes before cleanup\n\nI'm going to separate the cleanup to another Jenkins stage and force it to run to avoid this :+1:",
      "time": "20:08",
      "timestamp": "1638504518.260500",
      "is_reply": true
    },
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "Hmm, we should initiate the teardown in the `always` section inside `post {}`",
      "time": "20:09",
      "timestamp": "1638504561.260700",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "yep - was planning on throwing it onto `always` or using `catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {` to avoid stopping the build on stage failure (using this to clean up our non-K8s integration tests since we need those cleaned up before K8s integration tests can run)\n\nbut since this is at the end of the pipeline, `always` should be easier :+1:",
      "time": "20:10",
      "timestamp": "1638504656.260900",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "ah whoops, forgot this - `kind` is only installed in the Jenkins image which we're running on the build machine. As such, the step to delete the cluster needs to be run _inside_ the container on the build machine as well (can't be run directly in groovy script on build machine).\nWill think about this a bit more and figure out a nice way to do this (if kept in the post-steps, we'd expect the docker image to have been removed by then; relying on manual cleanup instead isn't ideal).",
      "time": "20:44",
      "timestamp": "1638506698.261500",
      "is_reply": true
    },
    {
      "sender": "kunal",
      "user_id": "U019YB70B8U",
      "message": "I think the motivation of running kind inside the jenkins container is so we can avoid clean up right? I’m guessing these old kind containers are from your old testing artifacts :slightly_smiling_face:",
      "time": "20:47",
      "timestamp": "1638506825.263400",
      "is_reply": true
    },
    {
      "sender": "advait",
      "user_id": "U02EBF84QKB",
      "message": "oh yes, that's also true - I had been installing minikube/kind/k3s/k3d directly onto the build machines before we'd changed that approach :sweat_smile:\n\nWe do share ports with the build machine due to the network access that Kind needs but yep I don't think the cluster _should_ be allowed to live after the container is removed.",
      "time": "20:49",
      "timestamp": "1638506945.263600",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2021-12-02.json",
    "message_count": 16,
    "start_time": "1638502438.258300",
    "end_time": "1638506945.263600",
    "is_thread": true
  }
}