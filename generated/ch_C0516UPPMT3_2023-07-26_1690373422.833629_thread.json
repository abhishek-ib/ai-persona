{
  "id": "ch_C0516UPPMT3_2023-07-26_1690373422.833629_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "vineeth",
    "Varun Jain",
    "Josh Heidebrecht",
    "Saikat"
  ],
  "messages": [
    {
      "sender": "Saikat",
      "user_id": "U017S746NKV",
      "message": "Is there any option to specify a `role` type in the prompt or changing the `temperature` when making the udf call to the llm models?",
      "time": "05:10",
      "timestamp": "1690373422.833629",
      "is_reply": false
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "No we dont provide those customizations as of now",
      "time": "08:30",
      "timestamp": "1690385435.267739",
      "is_reply": true
    },
    {
      "sender": "Saikat",
      "user_id": "U017S746NKV",
      "message": "Oh okay. Do we have any plans to introduce those in the newer releases?",
      "time": "08:31",
      "timestamp": "1690385505.006129",
      "is_reply": true
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "not right now. maybe @Hari / @Varun Jain can answer this better?",
      "time": "08:32",
      "timestamp": "1690385544.740729",
      "is_reply": true
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "What problem / use case are you trying to solve for, @Saikat ?",
      "time": "08:36",
      "timestamp": "1690385782.710279",
      "is_reply": true
    },
    {
      "sender": "Saikat",
      "user_id": "U017S746NKV",
      "message": "I was under the impression, if we can let the model know it’s “role: system” as let’s say a broker agent for broker presentation and return the responses accordingly, would we get a better result.\n\nAs for the temperature, for most cases I’d want to set the temperature to 0, but on some occasions if it’s for a summary I can go with a little higher temperature. Here (https://instabase.slack.com/archives/C05H7PDU1L5/p1690372819460429?thread_ts=1690365014.966439&cid=C05H7PDU1L5), we noticed inconsistent responses - so if we had the option to tweak temperature we could have had a more stable response?\n\nThese are not hard requests, but I was just wondering if us developers have that level of finer control while developing the solutions.",
      "time": "08:39",
      "timestamp": "1690385950.701699",
      "is_reply": true
    },
    {
      "sender": "Josh Heidebrecht",
      "user_id": "U02CL5VQL3S",
      "message": "@Varun Jain One situation where I believe temperature may play a role is financial documents (e.g. SEC filings like a 10-k).\n\nIe. many tables show numbers in millions. Asking for a value from the table like revenue on our platform gives the exact value in the table, e.g. ‘50’. But, asking the same question to other platforms GPT understands that the 50 is really 50 million and returns ‘50,000,000’. My hypothesis is that we’ve turned the temperature down so GPT returns the exact value, instead of applying that little bit of reasoning.",
      "time": "08:58",
      "timestamp": "1690387118.930219",
      "is_reply": true
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "Temperature set to non zero values has problems of hallucinations. So we avoid it",
      "time": "08:59",
      "timestamp": "1690387156.675609",
      "is_reply": true
    },
    {
      "sender": "Josh Heidebrecht",
      "user_id": "U02CL5VQL3S",
      "message": "@vineeth you may want to re-run your tests. I’ve found the latest chatgpt4 update has fixed almost all of the hallucination examples that I’ve tried. I’m preparing some material and wanted to find examples of GPT hallucinating and it’s been a challenge, so if you have an example in your tests where it hallucinates that would be awesome to share.",
      "time": "09:04",
      "timestamp": "1690387440.567729",
      "is_reply": true
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "By hallucinations I mean randomness. We try to keep the answers consistent when you ask the same set of questions across model runs. The official API page of openai recommends temperature 0 for deterministic results https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature (https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature)",
      "time": "09:07",
      "timestamp": "1690387635.115859",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-07-26.json",
    "message_count": 10,
    "start_time": "1690373422.833629",
    "end_time": "1690387635.115859",
    "is_thread": true
  }
}