{
  "id": "ch_C0516UPPMT3_2024-12-10_1733824266.861009_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "lydia",
    "Sławek Biel"
  ],
  "messages": [
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "Yes, the current logic for reasoning (and Converse) provenance is to do fuzzy string matching between the response and the document chunks with some heuristic on top of it to find a minimal rectangle that contains the relevant text.\n\nIt’s been implemented way back in the early aihub days by Rafał under constraints that it should be fast and not incur extra costs. It might make sense to revisit it today and find another approach, likely involving an additional LLM call.",
      "time": "01:51",
      "timestamp": "1733824266.861009",
      "is_reply": false
    },
    {
      "sender": "lydia",
      "user_id": "UJK4LKYSJ",
      "message": "@Kaustubh (KD) based on Slawek's summary, I think we can also tackle this with the other provenance improvements we were thinking for extraction (e.g. improving fuzzy string matching logic)",
      "time": "06:41",
      "timestamp": "1733841694.537999",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-12-10.json",
    "message_count": 2,
    "start_time": "1733824266.861009",
    "end_time": "1733841694.537999",
    "is_thread": true
  }
}