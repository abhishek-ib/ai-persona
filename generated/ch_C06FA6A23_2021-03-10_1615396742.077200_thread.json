{
  "id": "ch_C06FA6A23_2021-03-10_1615396742.077200_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Mohit",
    "mimi"
  ],
  "messages": [
    {
      "sender": "mimi",
      "user_id": "U011XR7S18U",
      "message": "responding to @Mohit from eng allhands here cuz it's REALLY important to clarify -- yes to \"fresh pair of eyes seeing product and giving feedback on usability\". This is the problem that usability testing (user research that centers around the theme of \"show users a mock/product, ask them to complete certain tasks, observer where their struggle) targets, and is highly highly highly important and your local PM/designers should be working with you to do this for anything you ship. This is not a debate -- everyone at this company should believe this is helpful, otherwise ping me and i can send you 1 billion research on this.\n\nthe discussion i think is more interesting is on - how do we _measure progress_ towards usability. this comes down to how you define \"usability\". is it:\n1. how we define usability in company direction? in which case \"usability\" means \"intermediate developer is able to create e2e solutions on their own\", and encapsulates not just small bug fixes, but lots of feature/experience adds to our product. this should be tracked via EPD OKRs, since all platform teams are driving towards this in H1\n2. how we define usability in konmari week? in which case \"usability\" means \"small, incremental bug fixes that make user experience a bit less confusing\".  there's still prioritization to be done within these, but i don't have qualms with measuring # of tickets as long as we are not picking all easy-to-fix but not-impactful usability bugs to tackle, to optimize for # of tickets\n3. breaking bugs? in which case, given how much product surface we cover, we first need to identify what we care to not break aka prioritize what user flows we care about the most (this is likely only a portion of urgent customer asks, because it needs to align with where we want our product to go towards), then track bugs within that category. here also, # of bugs can be a good measurement as long as we have guardrail in place\n4. not shipping breaking product to begin with? in which case, heymian and hari are working on testing framework & QA ramp up, and as part of that hopefully we will have holistic metrics to measure stability at a base level. However, every single team should be working with your PMs/EMs to ensure features shipped have a test plan, are tested, and the team reserves time for polish/bug fixing for when feature goes to internal and first external customer release. this can be tracked via # of breaking bugs reported _after_ feature being shipping (but not # of bugs resolved)\nprobably too heavyhanded of a response to what we were discussing, but this stuff is super important, so glad we are having this discussion and wanted to write these down to clarify what \"usability\" we are talking about here, cuz we discussed a lot of different flavors of them during the allhands.",
      "time": "09:19",
      "timestamp": "1615396742.077200",
      "is_reply": false
    },
    {
      "sender": "Mohit",
      "user_id": "ULPBBF8PR",
      "message": "makes sense, in my mind 2 is subset of 1. Thanks for clarification. this is really helpful and really important.",
      "time": "09:23",
      "timestamp": "1615397000.077400",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2021-03-10.json",
    "message_count": 2,
    "start_time": "1615396742.077200",
    "end_time": "1615397000.077400",
    "is_thread": true
  }
}