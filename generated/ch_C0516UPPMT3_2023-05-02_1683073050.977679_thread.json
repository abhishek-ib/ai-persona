{
  "id": "ch_C0516UPPMT3_2023-05-02_1683073050.977679_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Xi Cheng",
    "Seth",
    "Varun Jain"
  ],
  "messages": [
    {
      "sender": "Seth",
      "user_id": "U02TKEG137T",
      "message": "Does AIhub use reserved words? I entered a single word and it interpreted it as a detailed question.",
      "time": "17:17",
      "timestamp": "1683073050.977679",
      "is_reply": false
    },
    {
      "sender": "Seth",
      "user_id": "U02TKEG137T",
      "message": "Building off of this. I asked it to translate the document into French. Which it seemed “ok” at doing.  But then asked it to provide the answer in English (I wanted to see if it would translate it’s previous French answer into English (not just dump the english contents of the doc), but got this repeated answer:",
      "time": "17:35",
      "timestamp": "1683074132.090829",
      "is_reply": true
    },
    {
      "sender": "Seth",
      "user_id": "U02TKEG137T",
      "message": "It seems like it’s got some historical Q/A that it keeps repeating.",
      "time": "17:35",
      "timestamp": "1683074153.616099",
      "is_reply": true
    },
    {
      "sender": "Xi Cheng",
      "user_id": "U01F946DGEP",
      "message": "I think I heard that we have some logics that tries to \"cache\" answers so-to-speak to avoid sending every query to openAI to get an answer. @Rakesh can probably clarify the expected/unexpected behaviors here.",
      "time": "17:38",
      "timestamp": "1683074300.635569",
      "is_reply": true
    },
    {
      "sender": "Seth",
      "user_id": "U02TKEG137T",
      "message": "@Rakesh / @Varun Jain, what was the reasoning behind caching? How would AIHub know that the question that I’m asking would result in an answer that has already been provided?  I could be changing one word in my question to do prompt engineering and I wouldn’t want a cached answer.",
      "time": "17:56",
      "timestamp": "1683075382.948299",
      "is_reply": true
    },
    {
      "sender": "Xi Cheng",
      "user_id": "U01F946DGEP",
      "message": "@Seth Please take what I said with a grain of salt as I might have spread inaccurate info here. But I think a common problem we are facing is calling openAI endpoints too many times, and that adds pressure to the system and to the cost as well. So there is a desire to reduce that",
      "time": "18:37",
      "timestamp": "1683077835.439699",
      "is_reply": true
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "We are only using cached queries for sample project for providing an interactive onboarding experience to users in loginless state. \n\n@Seth @Xi Cheng",
      "time": "20:24",
      "timestamp": "1683084262.271619",
      "is_reply": true
    },
    {
      "sender": "Seth",
      "user_id": "U02TKEG137T",
      "message": "@Varun Jain can someone provide some insight into where the model is _*hallucinating the question*_ and then providing an answer are coming from? I’ve attached the document here.",
      "time": "23:14",
      "timestamp": "1683094485.608039",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-05-02.json",
    "message_count": 8,
    "start_time": "1683073050.977679",
    "end_time": "1683094485.608039",
    "is_thread": true
  }
}