{
  "id": "ch_C0516UPPMT3_2024-11-01_1730487113.634039_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Brian",
    "Serena"
  ],
  "messages": [
    {
      "sender": "Brian",
      "user_id": "U06U3PWKM0T",
      "message": "I'm going through the *initial* steps of configuring my ground-truth datasets for a document from which a table was extracted. However, the table extraction performed poorly and only resulted in 2 rows (SS1.)\n\nHowever, this was odd, since during app configuration, the extraction fared pretty well (SS2)\n\nI know the ground-truth dataset extractions are basically a re-run of the app, so performance may differ slightly between runs, BUT, I'm wondering: *can we incorporate the ability to re-run apps, maybe even for certain fields/extracted values for specific documents during the dataset configuration page?*\n• Otherwise, I'd have to manually construct the table or do re-runs (which would mean I'd have to re-establish the ground truth values for the other tables I've already went through)",
      "time": "11:51",
      "timestamp": "1730487113.634039",
      "is_reply": false
    },
    {
      "sender": "Brian",
      "user_id": "U06U3PWKM0T",
      "message": "Or maybe we can re-run the app, copy/download the extracted table/result, and *have a way to upload that as the values during review/GT value config?*",
      "time": "11:58",
      "timestamp": "1730487521.294189",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "when you rerun a ground truth set because it’s outdated, we maintain the ground truth values you previously set in Human Review",
      "time": "11:59",
      "timestamp": "1730487554.544849",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "this isn’t exactly what you’re looking for, but you could also run an accuracy test with your ground truth set (you’ll need to mark all documents as reviewed, though) since an accuracy test will also run the app again\n\nif you see a mismatch between the accuracy test’s field value and the ground truth set’s field value, you can click the expand icon and click “Set as ground truth” (see screenshots)",
      "time": "12:14",
      "timestamp": "1730488448.177469",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "to summarize, there are 2 issues here\n1. field values in Build projects vs app runs may not be consistent (tracked in https://instabase.atlassian.net/browse/EPD-2900)\n2. bc of this ^, sometimes users want to run re-extraction in Human Review, even if they didn’t change any class labels\n    a. do we want to create a new feature request for this? i think we should focus on addressing 1 instead\ncc @hannah",
      "time": "12:17",
      "timestamp": "1730488623.445949",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-11-01.json",
    "message_count": 5,
    "start_time": "1730487113.634039",
    "end_time": "1730488623.445949",
    "is_thread": true
  }
}