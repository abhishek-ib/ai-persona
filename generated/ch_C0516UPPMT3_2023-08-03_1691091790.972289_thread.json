{
  "id": "ch_C0516UPPMT3_2023-08-03_1691091790.972289_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Serena",
    "Dan H"
  ],
  "messages": [
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Hello -\n\nIt looks like we are formatting the input.  I tried to ask for a table and got some wonkiness.\n\n```Extract the data from this document as a table.  Include the following columns: \nNamed Insured\nc/o\nPrior Policy #\nCurrent Pol #\nEff Date\nAddress\nCity\nSt\nZip\nLocation Number\tAdded after all location descriptions are entered\nStreet Address\tMultiple lines cannot be inserted in cell\nCity \t\nST\t\nZip\t\nCountry\t- Defualt to United States\nPC\t- Protection Class\n# of Bldgs - Number of buildings at this location\n# Stys -Number of stories\nConst - Construction Type. Must match Intact - see Constr Type tab\n\nAS - Automatic Sprinkler\nYear Built\t\nYear Updated\t\nSq Ft - Square Footage of building\nBldg - Building limit\nCTs - Contents limit\nBI - Business Income limit\nRts - Rental Income\nEE - Extra Expense\nOther - Other coverage limits```",
      "time": "12:43",
      "timestamp": "1691091790.972289",
      "is_reply": false
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "This is a frontend issue - we currently use Markdown to format the prompt, so the frontend interprets a line that starts with `#` as a header (ex: `# of Bldgs`) cc @Subash\n\nbtw which environment is this?",
      "time": "13:11",
      "timestamp": "1691093516.350539",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "This is prod",
      "time": "14:10",
      "timestamp": "1691097004.060459",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Additionally, the chunking failed in this case.  We are grabbing an excel doc and trying to normalize the data.  The first full page and a half is relevant, but (if prov tracking is to be trusted) our chunking only caught the first row.  The tool also only returned 1 row:",
      "time": "14:11",
      "timestamp": "1691097100.405509",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "i think @Rakesh is already working on better provenance support for long tables for 23.08",
      "time": "14:12",
      "timestamp": "1691097169.161449",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-08-03.json",
    "message_count": 5,
    "start_time": "1691091790.972289",
    "end_time": "1691097169.161449",
    "is_thread": true
  }
}