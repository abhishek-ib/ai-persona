{
  "id": "ch_C0516UPPMT3_2024-10-09_1728510665.586919_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Nathaniel"
  ],
  "messages": [
    {
      "sender": "Nathaniel",
      "user_id": "U04BGHM4AEL",
      "message": "@Kaustubh (KD) Certainly!\n• I can't seem to reproduce it now, but when I was trying out this feature on the evening of Oct. 1st, the `Tax year` field on 1040's (ss1) wasn't generating super helpful prompts.  It was something to the effect of `OMB attachment sequence`.  Sample file attached.\n• The naming conventions might vary from client to client, depending on what systems we're working with.\n    ◦ A generally useful naming feature would be the addition of more special characters like commas, dashes, or periods.\n    ◦ A convention that might be common would be referring to fields by their ordinal designation, such as `Part 1, Line 8j` (as opposed to `Activity not engaged in profit income` ).  Ordinal naming sometimes works well, and sometimes performs more poorly than the more word based names that we can tweak to prompt the model more clearly.\n    ◦ Yes a `downstream systems name` would be helpful, but that same name would likely also be helpful as the main display name for the field/schema in the build project too.  Whatever downstream naming convention we're using would also likely be helpful for keeping the various fields organized/cleanly readable within Build too.\n• I don't think any changes/issues with derived fields back-propigate to the source linage.  \n    ◦ However, the UX isn't great when there are a lot of fields and on top of that we only care about reviewing every 2nd or 3rd field.  The human reviewer shouldn't have to scroll past/ignore fields that aren't getting utilized in the final output, as making them do that takes more time, effort, and can lead to things getting missed.\nNo prob @Vishnu!\n• It's not necessarily that using the existing field names and descriptions is making the generated prompt worse than if we started with a blank field name+description.  The concern that I was raising is that if we generate a field name and description, the generated one might be worse than the ones that we started with.\n    ◦ If comparing generated prompt vs generated prompt, the one generated from a reasonable starting name+description seems to always be better than the one generated from a blank name+description.\n    ◦ What I'm saying is that comparing the original manually-written name+description vs the ones generated from them, can feel like a downgrade.  So it would be nice to have a way to non-committally generate a name+description to see *if* the model can improve the prompt you refined, but then if it's not an improvement make it easy to switch back to what you had before.",
      "time": "14:51",
      "timestamp": "1728510665.586919",
      "is_reply": false
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-10-09.json",
    "message_count": 1,
    "start_time": "1728510665.586919",
    "end_time": "1728510665.586919",
    "is_thread": true
  }
}