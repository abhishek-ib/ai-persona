{
  "id": "ch_C06FA6A23_2023-12-20_1703102351.663709_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Anant",
    "donagh"
  ],
  "messages": [
    {
      "sender": "donagh",
      "user_id": "U036AEB3AF9",
      "message": "From one of the engineers that worked on the Stripe x OpenAI collaboration that pioneered a commercialized RAG implementation (used for searching their extensive docs). Might be of interest to folks here.  https://blog.elicit.com/search-vs-vector-db/",
      "time": "11:59",
      "timestamp": "1703102351.663709",
      "is_reply": false
    },
    {
      "sender": "Anant",
      "user_id": "U0U100MNZ",
      "message": "@donagh Shaunak, Kerry, Clemens, and I actually spent a pretty good amount of time actually discussing this (solving the search problem vs LLM over Vector DB problem) couple of weeks ago, and why we need a classic search index (the way perplexity is solving this problem) in certain clients vs. just RAGs at each client.\n\n*I would highly recommend everyone to read this.*",
      "time": "14:46",
      "timestamp": "1703112368.057219",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2023-12-20.json",
    "message_count": 2,
    "start_time": "1703102351.663709",
    "end_time": "1703112368.057219",
    "is_thread": true
  }
}