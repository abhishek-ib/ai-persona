{
  "id": "ch_C06FA6A23_2024-04-29_1714384726.419679_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Vishnu"
  ],
  "messages": [
    {
      "sender": "Vishnu",
      "user_id": "U02RY7NLQMN",
      "message": "> Some LLMs like Claude from Anthropic and GPT4 have started providing extremely large context sizes (100K tokens for Claude, 128K tokens for GPT4). However, experimental evaluations of text generated by these LLMs indicate that the accuracy and recall of these LLMs with larger context is lower, keeping the parameter size constant. This again is explained by our model. If the LLM is attempting a compact representation of the large probability matrix, then going from a context size of 8000 (GPT 3.5) to 128 is an enormous increase in the row size of the matrix (from 500008000 to 50000128000) and it is not surprising that the models are unable to retain the full context, the posterior computation happening over a much larger observable variable. The approximation needed becomes a lot more difficult for the LLMs handle.\nthis is pretty evident and why we see RAG and map-reduce with smaller models to be much more effective. would be interesting to see if anyone is focussing specifically on making large contexts work and overcoming the approximation problem",
      "time": "02:58",
      "timestamp": "1714384726.419679",
      "is_reply": false
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2024-04-29.json",
    "message_count": 1,
    "start_time": "1714384726.419679",
    "end_time": "1714384726.419679",
    "is_thread": true
  }
}