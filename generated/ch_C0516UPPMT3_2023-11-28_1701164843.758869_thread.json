{
  "id": "ch_C0516UPPMT3_2023-11-28_1701164843.758869_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Hari",
    "Atinder",
    "Varun Jain",
    "Matt Weaver",
    "Sławek Biel"
  ],
  "messages": [
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "Hey team,\nI've been trying a few very challenging examples with multi-file on prod and running into a few issues.\nThis is just me playing around, but I thought the example could be useful for our internal testing",
      "time": "01:47",
      "timestamp": "1701164843.758869",
      "is_reply": false
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Thanks for the feedback, @Matt Weaver! We will check these files, and figure out how to improve the performance for these complex queries. cc: @Hari @Atinder",
      "time": "01:55",
      "timestamp": "1701165304.965609",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "Here’s what I got with multistep",
      "time": "02:07",
      "timestamp": "1701166067.572859",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "Multistep does not work with multi-file on prod right now?",
      "time": "02:09",
      "timestamp": "1701166183.013649",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "It’s targeted for 23.49",
      "time": "02:10",
      "timestamp": "1701166201.962559",
      "is_reply": true
    },
    {
      "sender": "Hari",
      "user_id": "UCX3XL72Q",
      "message": "@Matt Weaver Can you try it in internal sandbox? you can try both advanced and multi-step here, they are the latest.",
      "time": "02:10",
      "timestamp": "1701166252.573759",
      "is_reply": true
    },
    {
      "sender": "Atinder",
      "user_id": "U019P6NA1RN",
      "message": "The improvements related to comparative questions in our baseline multidoc are coming soon, I will update here",
      "time": "02:11",
      "timestamp": "1701166304.450929",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "Awesome, thanks!",
      "time": "02:15",
      "timestamp": "1701166533.633659",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "How expensive do you think this query is to run? :sweat_smile:",
      "time": "02:16",
      "timestamp": "1701166578.021889",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "Around $0.30 was our median cost on the benchmark and we’d charge 180 units based on the total pages count",
      "time": "02:20",
      "timestamp": "1701166853.489059",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "30c is median cost for multi-step gpt-4?",
      "time": "02:21",
      "timestamp": "1701166915.554699",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "Does the cost increase if there are more documents or pages? Or we submit rightly the same tokens each time?",
      "time": "02:22",
      "timestamp": "1701166940.709349",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "The range is pretty wide from $0.07 to %1.50, depends more on the question itself and how quickly model was able to find the relevant information, and how many pages it needed to consume.",
      "time": "02:24",
      "timestamp": "1701167092.513459",
      "is_reply": true
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "@Matt Weaver - Multistep is not deterministic, the costs depend on how many steps agent comes up with to solve the query. \n\nWe will publish the benchmark for multi doc queries soon. Slawek referred to the single large doc benchmark here I believe.",
      "time": "03:00",
      "timestamp": "1701169216.022959",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "I got a different result than @Sławek Biel when trying on internal ?",
      "time": "04:31",
      "timestamp": "1701174696.932599",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "I’ve found the trace with your run, but don’t see anything suspicious there. Gpt-4 is not deterministic in it’s answers. I just tried couple more times the same question and got different variations",
      "time": "05:04",
      "timestamp": "1701176640.510619",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "have we experimented with this?\nhttps://cobusgreyling.medium.com/now-you-can-toggle-openai-model-determinism-8b661e02cf98",
      "time": "07:14",
      "timestamp": "1701184464.004059",
      "is_reply": true
    },
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "I think variability in outputs could make many enterprise customers very nervous",
      "time": "07:15",
      "timestamp": "1701184510.416549",
      "is_reply": true
    },
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "The seed paramater is only present in the newest OpenAI models. We’d experiment more once the gpt-4-turbo has enough throughput to use in prod",
      "time": "07:21",
      "timestamp": "1701184890.035929",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-11-28.json",
    "message_count": 19,
    "start_time": "1701164843.758869",
    "end_time": "1701184890.035929",
    "is_thread": true
  }
}