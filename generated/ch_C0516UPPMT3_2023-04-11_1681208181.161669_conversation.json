{
  "id": "ch_C0516UPPMT3_2023-04-11_1681208181.161669_conversation",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "conversation",
  "participants": [
    "Varun Jain"
  ],
  "messages": [
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Hi Tom - For the enterprise offering, we would allow customers to mount their filesystems / drives. Using chat app, they would be able to converse across a large corpus of documents.",
      "time": "03:16",
      "timestamp": "1681208181.161669",
      "is_reply": false
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "We have also seen interest in auto extraction from a large repo of documents in a structured database (think snowflake / databricks) - proposal here https://docs.google.com/document/d/1RUHCaYAOESt1nMoevQbdCeuc1aiKGkv_wMA8SGVgerg/edit#",
      "time": "03:19",
      "timestamp": "1681208382.930829",
      "is_reply": false
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Scaling the system across millions of documents and high number of concurrent users is possible. It’d involve couple of things at a high level:\n• Our infra teams would need to understand the requirements of the customers and help size the infra accordingly. \n• We’d work with the LLM vendors (OpenAI / Azure) to make sure that their API rate limits are not a blocker.",
      "time": "03:22",
      "timestamp": "1681208521.045239",
      "is_reply": false
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-04-11.json",
    "message_count": 3,
    "start_time": "1681208181.161669",
    "end_time": "1681208521.045239",
    "is_thread": false
  }
}