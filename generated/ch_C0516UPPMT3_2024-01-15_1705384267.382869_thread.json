{
  "id": "ch_C0516UPPMT3_2024-01-15_1705384267.382869_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Varun Jain"
  ],
  "messages": [
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "@Rakesh @Hari - Which GPT-3.5 turbo model version are we using currently? Have we moved to the latest version (1106) which supports 16K context window, with 4K tokens for output? https://platform.openai.com/docs/models\n\nIâ€™m seeing our customers (e.g. Ocrolus) trying to extract tables in a specific downstream format which is getting limited due to context window.\n\ncc: @Tom",
      "time": "21:51",
      "timestamp": "1705384267.382869",
      "is_reply": false
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "There is no impact on the pricing and rate limits from this decision",
      "time": "21:53",
      "timestamp": "1705384421.837169",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-01-15.json",
    "message_count": 2,
    "start_time": "1705384267.382869",
    "end_time": "1705384421.837169",
    "is_thread": true
  }
}