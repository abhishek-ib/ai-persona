{
  "id": "ch_C06FA6A23_2021-08-11_1628709165.104300_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Kerry",
    "sudeep",
    "Shubham Oli",
    "shaunak",
    "Dan H"
  ],
  "messages": [
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Seems we are getting 504 Errors on IB.com (http://IB.com) again, can anyone take a look?",
      "time": "12:12",
      "timestamp": "1628709165.104300",
      "is_reply": false
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Looking now.",
      "time": "12:17",
      "timestamp": "1628709430.104400",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Can you let us know what app you are using while you get these errors?",
      "time": "12:17",
      "timestamp": "1628709459.104600",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Yes! I Think this was pulling up a flow and flow review",
      "time": "12:48",
      "timestamp": "1628711285.105100",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "OK, @Kerry can you help here?  We are seeing fairly regular crashes in apps-server.",
      "time": "12:49",
      "timestamp": "1628711376.105300",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "@Dan H it would be good to understand the flow-review page that you are trying to load",
      "time": "12:49",
      "timestamp": "1628711397.105500",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "that might help Kerry and team get to the bottom of these issues.",
      "time": "12:50",
      "timestamp": "1628711408.105800",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Sure. I actually loaded all the pages for a demo concurrently this time, and everything else worked ok. But I was seeing lots of slowness opening other stuff too. I’ll share that link as well as the others in just a minute",
      "time": "12:51",
      "timestamp": "1628711469.107700",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "yep sounds good - please share the link here @Dan H, we’ll look into it",
      "time": "12:58",
      "timestamp": "1628711880.107900",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "@Kerry this is an interesting case where even though the gcloud console is not reporting any restarts, we are seeing restarts of the pod on grafana",
      "time": "13:01",
      "timestamp": "1628712060.108200",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Gcloud (https://console.cloud.google.com/kubernetes/pod/us-central1-c/instabase-main-prod-002/instabase-prod/deployment-apps-server-84758ffccd-td48j/details?organizationId=396394272939&project=instabase-main) v/s Grafana (https://instabase.com/grafana/d/6581e46e4e5c7ba40a07646395ef7b23/kubernetes-compute-resources-pod?var-datasource=1.%20Victoriametrics&var-cluster=&var-namespace=instabase-prod&var-pod=deployment-apps-server-84758ffccd-td48j&orgId=1&refresh=10s)",
      "time": "13:01",
      "timestamp": "1628712109.108400",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "This is the flow which failed: https://instabase.com/apps/flow/edit/SA_team/demo/fs/Instabase%20Drive/Consumer%20Lending/workflows/PDF%20Packet.ibflow\n\nI _think_ this was the flow review:\nhttps://instabase.com/SA_team/demo/fs/Instabase%20Drive/Custom%20Demos/Northwestern%20Mutual/AttendingPhysicianStatement/out/batch.ibflowresults",
      "time": "13:05",
      "timestamp": "1628712334.108700",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "@Dan H how did you know if it’s a 504? like, for example, the toast shows a 504? you look at the network tab and found some API call returns 504? or the whole page shows nginx error 504?",
      "time": "13:07",
      "timestamp": "1628712438.108900",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "also @Dan H, can you let us know around what time the 504 happened?\n\n@shaunak - just to learn more about grafana, how did you tell in the grafana link that the apps server restarts? like, memory usage goes to 0 at 15:15 and 15:56 (like in the screenshot?) Any idea why GCloud doesn’t report alerts?",
      "time": "13:09",
      "timestamp": "1628712585.109100",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "",
      "time": "13:09",
      "timestamp": "1628712599.109500",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "It probably happened sometime around 3 -3:10 i think",
      "time": "13:10",
      "timestamp": "1628712621.109900",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "@Kerry FYI",
      "time": "13:10",
      "timestamp": "1628712635.110100",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "super helpful, thanks",
      "time": "13:11",
      "timestamp": "1628712719.110500",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Happy to!  Is this the best way to call these out? or do you need me to submit a something through Jira?  I wanna make sure im not continually mucking up your workflow :slightly_smiling_face:",
      "time": "13:13",
      "timestamp": "1628712821.110700",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "this is fine :slightly_smiling_face: so you were unable to reach the flow editor, we’ll do some investigation",
      "time": "13:25",
      "timestamp": "1628713548.114300",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "@Kerry Re: apps-server crashes, that is correct.  I also looked through the logs and definitely see missing liveness and readiness probes during the timeframe.  We are already in communication with the gcloud team on understand what the issue is.",
      "time": "14:02",
      "timestamp": "1628715755.114500",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "Hey yeah, so I did some quick investigation\n\n1.  Looking at the logs from apps server (https://console.cloud.google.com/logs/query;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22instabase-main%22%0Aresource.labels.location%3D%22us-central1-c%22%0Aresource.labels.cluster_name%3D%22instabase-main-prod-002%22%0Aresource.labels.namespace_name%3D%22instabase-prod%22%0Aresource.labels.container_name%3D%22apps-server%22;timeRange=2021-08-11T19:00:03.570Z%2F2021-08-11T19:11:03.570Z;cursorTimestamp=2021-08-11T19:06:00.276463386Z?organizationId=396394272939&project=instabase-main) between 2:55-3:15pm ET, I can see a bunch of alive.txt errors, along with several GET failure for Refiner, Flow, Flow Review app. I hid error messages that contain `alive.txt` and was able to find 2 (https://console.cloud.google.com/logs/query;cursorTimestamp=2021-08-11T18:58:06.538654815Z;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22instabase-main%22%0Aresource.labels.location%3D%22us-central1-c%22%0Aresource.labels.cluster_name%3D%22instabase-main-prod-002%22%0Aresource.labels.namespace_name%3D%22instabase-prod%22%0Aresource.labels.container_name%3D%22apps-server%22%0ANOT%20%22alive.txt%22%0A%22Consumer%22%0Atimestamp%3D%222021-08-11T18:56:34.739953594Z%22%0AinsertId%3D%22rhdog3d0kj323udy%22;timeRange=2021-08-11T18:50:03.570Z%2F2021-08-11T19:18:03.570Z?organizationId=396394272939&project=instabase-main) errors (one in apps-server (https://console.cloud.google.com/logs/query;cursorTimestamp=2021-08-11T18:58:06.538627815Z;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22instabase-main%22%0Aresource.labels.location%3D%22us-central1-c%22%0Aresource.labels.cluster_name%3D%22instabase-main-prod-002%22%0Aresource.labels.namespace_name%3D%22instabase-prod%22%0Aresource.labels.container_name%3D%22apps-server%22%0ANOT%20%22alive.txt%22%0A%22Consumer%22%0Atimestamp%3D%222021-08-11T18:56:34.739953594Z%22%0AinsertId%3D%22rhdog3d0kj323udy%22;timeRange=2021-08-11T18:50:03.570Z%2F2021-08-11T19:18:03.570Z?organizationId=396394272939&project=instabase-main), one in nginx (https://console.cloud.google.com/logs/query;cursorTimestamp=2021-08-11T19:01:04.419759634Z;query=resource.type%3D%22k8s_container%22%0Aresource.labels.project_id%3D%22instabase-main%22%0Aresource.labels.location%3D%22us-central1-c%22%0Aresource.labels.cluster_name%3D%22instabase-main-prod-002%22%0Aresource.labels.namespace_name%3D%22instabase-prod%22%0Aresource.labels.container_name%3D%22server-nginx%22%0Aseverity%3D%22Error%22%0A%22PDF%2520Packet.ibflow%22%0Atimestamp%3D%222021-08-11T19:01:04.419759634Z%22%0AinsertId%3D%22wokyswlbspcp80iy%22;timeRange=2021-08-11T18:55:23.949Z%2F2021-08-11T19:15:23.949Z?organizationId=396394272939&project=instabase-main)) that might be Daniel’s screenshot\n2. ~I went to the Nginx dashboard for prod. Looking at the time range from 2:55-3:05pm ET (https://instabase.com/grafana/d/ZQAsi-Xizz/nginx-dashboard?orgId=1&from=1628708040000&to=1628708743000), somehow there’s no 5xx errors recorded, there some 4xx~  Confirmed in our grafana dashboard that some 5xx errors happened during the time period. \nOne thing I can tell is this is probably not because of a particular app (like flow review) but some general unstableness on prod, as i also see same errors in apps-server for all frequently used apps around that time period: refiner, classifier, flow editor, flow run, flow review.\n\nBut I’m unsure how to move on next. @shaunak - any suggestions? Any observability tools that I should use?\n(I also posted this in flow’s team channel, we’ll do more investigation)",
      "time": "14:21",
      "timestamp": "1628716869.114700",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "@Kerry these are not `ERROR` messages.  These are messages that get logged at the `ERROR` level due to some StackDriver issues.  I think the apps-server became unusable due to it running very close to the CPU limit `200m`.   Can you try to run this in docker and limit the CPU to be 200m?  That might help us reproduce the issue locally.",
      "time": "15:04",
      "timestamp": "1628719481.115500",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "I sure can!",
      "time": "16:22",
      "timestamp": "1628724137.115900",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "looking at this a bit more (haven’t got to run this locally, @shaunak - just more questions :slightly_smiling_face: thanks for answering)\n1. So just a clarification question about the apps-server logs… all the `\"GET /static/alive.txt HTTP/1.1\" 200 310 0.001533` messages are not errors actually, they are just somehow logged at the error level? \n2. I’m looking at the CPU usage for the past 2 days for all the apps-server pods, actually only the pod that you shared (this one (https://instabase.com/grafana/d/6581e46e4e5c7ba40a07646395ef7b23/kubernetes-compute-resources-pod?var-datasource=1.%20Victoriametrics&var-cluster=&var-namespace=instabase-prod&var-pod=deployment-apps-server-84758ffccd-td48j&orgId=1&from=now-2d&to=now&refresh=10s)) has this CPU usage trend that the usage just keeps going up and now gets quite close to the 200m limit. All other pods have low usage (like this one (https://instabase.com/grafana/d/6581e46e4e5c7ba40a07646395ef7b23/kubernetes-compute-resources-pod?var-datasource=1.%20Victoriametrics&var-cluster=&var-namespace=instabase-prod&var-pod=deployment-apps-server-84758ffccd-lk27q&orgId=1&from=now-2d&to=now&refresh=10s)). Is that something we see often? Is it possible that we have an unhealthy pod?",
      "time": "18:56",
      "timestamp": "1628733412.116600",
      "is_reply": true
    },
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "@shaunak @Kerry I think there is some misconfiguration with Error levels in python apps. I’ve also seen the same behaviour in api-sever-apps where every log regardless of type gets published as Error in the stackdriver logs.",
      "time": "19:50",
      "timestamp": "1628736622.116900",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "yeah it’s quite annoying and misleading",
      "time": "19:54",
      "timestamp": "1628736891.117100",
      "is_reply": true
    },
    {
      "sender": "Kerry",
      "user_id": "UCX3VGDJR",
      "message": "@Shubham Oli would love your insights on my question 2: not sure what this means that 1 pod’s CPU usage looks so different than all other pods",
      "time": "19:56",
      "timestamp": "1628736996.117400",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Yea, the python app logs are very annoying :disappointed: We should fix them.  @Kerry you are right.  Looks like the apps-server is in an unhealthy state.  I’m not sure we have the correct tools to debug what might be going on here.   We can restart the pod to get it back to the normal behaviour.",
      "time": "21:20",
      "timestamp": "1628742040.119700",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Other thing that is interesting is that this is the only pod that hasn’t restarted.  All other pods have restarted and have had their CPU reset back.  @Shubham Oli can you please quarantine this pod away so that we can do deeper investigation for later.  May be @sudeep, @kunal and @Aaron Tami might be interesting in looking at it.",
      "time": "21:30",
      "timestamp": "1628742636.119900",
      "is_reply": true
    },
    {
      "sender": "sudeep",
      "user_id": "UCWRN72EP",
      "message": "hey - i did start to look at this more closely, and as per our stats do not see any restarts though. @shaunak can you point me to readiness probe failures that you saw\n\nhttps://instabase.com/grafana/explore?orgId=1&left=%5B%22now-12h%22,%22now%22,%221.[…]5C%22deployment-apps-server-84758ffccd-td48j%5C%22%7D%22%7D%5D (https://instabase.com/grafana/explore?orgId=1&left=%5B%22now-12h%22,%22now%22,%221.%20Victoriametrics%22,%7B%22exemplar%22:true,%22expr%22:%22kube_pod_container_status_ready%7Bpod%3D%5C%22deployment-apps-server-84758ffccd-td48j%5C%22%7D%22%7D%5D)",
      "time": "21:33",
      "timestamp": "1628742806.120100",
      "is_reply": true
    },
    {
      "sender": "Shubham Oli",
      "user_id": "U01CKNF7BSL",
      "message": "> can you please quarantine this pod away\ndone",
      "time": "21:36",
      "timestamp": "1628742964.122200",
      "is_reply": true
    },
    {
      "sender": "shaunak",
      "user_id": "UCY6SA014",
      "message": "Great, thanks!",
      "time": "21:36",
      "timestamp": "1628742998.122500",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2021-08-11.json",
    "message_count": 33,
    "start_time": "1628709165.104300",
    "end_time": "1628742998.122500",
    "is_thread": true
  }
}