{
  "id": "ch_C0516UPPMT3_2023-09-05_1693923091.367399_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Matt Macnak",
    "Travis",
    "Dan H"
  ],
  "messages": [
    {
      "sender": "Travis",
      "user_id": "U017E5NNKAP",
      "message": "Is this aihub or flow review?",
      "time": "07:11",
      "timestamp": "1693923091.367399",
      "is_reply": false
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "Wow terrible reporting on my part.  This is using `call_model_converse()` with the ibllm implementation for the enterprise platform. `23.07.8`\nhttps://sandbox.console.instabase.com/customer/Customer%20Success/tenant/intact-workshop/info",
      "time": "07:13",
      "timestamp": "1693923213.628419",
      "is_reply": true
    },
    {
      "sender": "Travis",
      "user_id": "U017E5NNKAP",
      "message": "Ah so that should be an exception error and not the body text of the field.",
      "time": "07:15",
      "timestamp": "1693923312.234499",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "So i think there are a few takeaways:\n• This should be an exception error\n• I'd like to see us delay and retry rather than just giving up (production solutions wouldnt be able to support the giveup)\n• It may be helpful to allow us to control how much context goes into the model.  For example, I know @Matt Macnak and myself and a few other SEs have come across the need to call the LLM without any document context.  Theres ocassionally a load of wasted tokens when I am calling `call_model_converse` and basically telling it to ignore the document attached.",
      "time": "07:18",
      "timestamp": "1693923505.271859",
      "is_reply": true
    },
    {
      "sender": "Matt Macnak",
      "user_id": "U02M5TNMH19",
      "message": "Agree - if we could have an API endpoint to call the LLM directly where we provide the context + prompt that would be extremely helpful in refining our final outputs",
      "time": "08:37",
      "timestamp": "1693928221.473649",
      "is_reply": true
    },
    {
      "sender": "Travis",
      "user_id": "U017E5NNKAP",
      "message": "Within human review? Interesting idea",
      "time": "08:58",
      "timestamp": "1693929486.264979",
      "is_reply": true
    },
    {
      "sender": "Matt Macnak",
      "user_id": "U02M5TNMH19",
      "message": "No, in refiner but it could also be used there",
      "time": "08:58",
      "timestamp": "1693929503.100209",
      "is_reply": true
    },
    {
      "sender": "Travis",
      "user_id": "U017E5NNKAP",
      "message": "The idea of peppering the LLMs throughout enterprise I feel could be very powerful.",
      "time": "08:58",
      "timestamp": "1693929539.771569",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "So actually from the SE side, there are 2 seperate requests... (<#C05LLRV7VK3|> is my original point)\n\n1. Use llm directly from refiner\n2. Ability to interact & 'reprocess refinement' in human review (interact with converse could in theory be included in that),",
      "time": "08:59",
      "timestamp": "1693929567.554189",
      "is_reply": true
    },
    {
      "sender": "Dan H",
      "user_id": "U020E9AS9P0",
      "message": "^Lots of areas of opportunity across the platform to use LLM for validation, and for Refinement",
      "time": "09:00",
      "timestamp": "1693929600.096189",
      "is_reply": true
    },
    {
      "sender": "Travis",
      "user_id": "U017E5NNKAP",
      "message": "I think I’m refiner does make more sense than human review.",
      "time": "09:13",
      "timestamp": "1693930420.836739",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-09-05.json",
    "message_count": 11,
    "start_time": "1693923091.367399",
    "end_time": "1693930420.836739",
    "is_thread": true
  }
}