{
  "id": "ch_C0516UPPMT3_2023-06-01_1685607477.470439_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "joshbronko",
    "vineeth",
    "Rafal"
  ],
  "messages": [
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "Totally understand. I’m more curious on rough page count. Or if we have token size. Do we split on 4k tokens/words. Etc.",
      "time": "01:17",
      "timestamp": "1685607477.470439",
      "is_reply": false
    },
    {
      "sender": "vineeth",
      "user_id": "U0175SZ13F0",
      "message": "Its dynamic - its token count of doc chunk + token count of prompt (variable in size) + response tokens (700)",
      "time": "03:51",
      "timestamp": "1685616685.693839",
      "is_reply": true
    },
    {
      "sender": "Rafal",
      "user_id": "U023S84DFDJ",
      "message": "@joshbronko if you want to quickly check the size of the doc you could paste an INPUT_COL here\nhttps://platform.openai.com/tokenizer\nif it’s above 3300 it is likely a long doc\nif it’s significantly below and you don’t have a lot of fields then it’s probably a short doc",
      "time": "04:18",
      "timestamp": "1685618321.933889",
      "is_reply": true
    },
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "Perfect. That’s what I was looking for",
      "time": "04:32",
      "timestamp": "1685619154.788509",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-06-01.json",
    "message_count": 4,
    "start_time": "1685607477.470439",
    "end_time": "1685619154.788509",
    "is_thread": true
  }
}