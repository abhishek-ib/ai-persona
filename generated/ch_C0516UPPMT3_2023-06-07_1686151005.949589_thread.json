{
  "id": "ch_C0516UPPMT3_2023-06-07_1686151005.949589_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Hari",
    "Matt Weaver",
    "Lalit"
  ],
  "messages": [
    {
      "sender": "Matt Weaver",
      "user_id": "U01B8GFNUAC",
      "message": "Do we plan to handle context-window / token limits for long responses?\n\nI.e. i have a long bank-statement with a multi-page table, i try to extract the table as a json but it can never grab the whole thing",
      "time": "08:16",
      "timestamp": "1686151005.949589",
      "is_reply": false
    },
    {
      "sender": "Hari",
      "user_id": "UCX3XL72Q",
      "message": "yes! this is currently in dev to find ways to extract continuous tables across pages. Will be great if you can pass the doc to Lalit, if it's not customer data? cc: @Lalit",
      "time": "08:40",
      "timestamp": "1686152424.890749",
      "is_reply": true
    },
    {
      "sender": "Lalit",
      "user_id": "U038G74CGTA",
      "message": "Yeah, I'm looking at detecting and combining multi-page tables. Given the LLM's output token limit, extracting the entire table might require larger limit since some tables can span 50s of pages. In that case extracting relevant fields and summaries like total amount withdrawn on a particular date might make more sense.\nPlease share the doc if possible, I'll add it to our benchmark dataset.",
      "time": "21:53",
      "timestamp": "1686200015.411649",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-06-07.json",
    "message_count": 3,
    "start_time": "1686151005.949589",
    "end_time": "1686200015.411649",
    "is_thread": true
  }
}