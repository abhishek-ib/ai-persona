{
  "id": "ch_C0516UPPMT3_2025-01-07_1736263681.652959_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "jordy.vlan",
    "Raushan Kumar Pandey",
    "Serena",
    "Nathaniel"
  ],
  "messages": [
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "Yes, there is a constant `MODEL_TO_MAX_OUTPUT_TOKENS` that is set to 4K for most production LLMs.\nhttps://github.com/instabase/instabase/blob/f356fe8d921ff824315154ba8e43d69570fbdb[â€¦]/py-utils/ibllm/src/py/instabase/ibllm/model/tasks/constants.py (https://github.com/instabase/instabase/blob/f356fe8d921ff824315154ba8e43d69570fbdb2d/shared-utils/py-utils/ibllm/src/py/instabase/ibllm/model/tasks/constants.py#L254-L264)",
      "time": "07:28",
      "timestamp": "1736263681.652959",
      "is_reply": false
    },
    {
      "sender": "jordy.vlan",
      "user_id": "U072CDMB4N8",
      "message": "My advice would be to try and split this into multiple document reasoning fields if possible :confused:",
      "time": "07:30",
      "timestamp": "1736263828.231639",
      "is_reply": true
    },
    {
      "sender": "Raushan Kumar Pandey",
      "user_id": "U036AEARNRM",
      "message": "Its a build project, also the limit is for each field and not per document right?",
      "time": "08:04",
      "timestamp": "1736265852.754709",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "for reasoning fields, yes the limit is per field",
      "time": "08:05",
      "timestamp": "1736265921.072879",
      "is_reply": true
    },
    {
      "sender": "Nathaniel",
      "user_id": "U04BGHM4AEL",
      "message": "To help alert when this issue is encountered, might there be a way to notify/flag when a model response hits that `MODEL_TO_MAX_OUTPUT_TOKENS` limit?\n\nIdeally the message displayed could be clear enough to prompt a user to split up the field without needing any additional guidance.  But also, ideally the message would trigger some sort of error in a deployed solution so as to be caught by human review, should an exceptionally long response happen to a use case in production.  What do you all think?",
      "time": "13:58",
      "timestamp": "1736287132.829339",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2025-01-07.json",
    "message_count": 5,
    "start_time": "1736263681.652959",
    "end_time": "1736287132.829339",
    "is_thread": true
  }
}