{
  "id": "ch_C0516UPPMT3_2023-12-04_1701680724.949519_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Varun Jain",
    "Sławek Biel"
  ],
  "messages": [
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "Alright, the way our “long table extraction” pipeline works right now is pretty inflexible in terms of prompt. You need to use specific phrasing and it’s not possible to filter by name or description.\nThe reason your query worked in the default model is because it didn’t trigger the table pipeline at all - it send the document and the prompt to GPT and it generated the response, you’d get the same result without the “Object detection” toggle. The multistep tried to invoke the table pipeline but failed to match from the title.\n\nIf you use the prompt “extract all tables return in json format” that would use the table pipeline across all the models.\nThis will be changing soon as we are updating this to support more prompts and usecases.",
      "time": "01:05",
      "timestamp": "1701680724.949519",
      "is_reply": false
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Thanks for the feedback, Chris! As Slawek mentioned, we are working to make table extraction less prompt sensitive and will be releasing some improvements in the 23.49 release. cc: @Lalit @Rakesh",
      "time": "02:24",
      "timestamp": "1701685489.326459",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-12-04.json",
    "message_count": 2,
    "start_time": "1701680724.949519",
    "end_time": "1701685489.326459",
    "is_thread": true
  }
}