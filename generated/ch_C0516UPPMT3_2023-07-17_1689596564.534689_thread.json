{
  "id": "ch_C0516UPPMT3_2023-07-17_1689596564.534689_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Varun Jain",
    "avi",
    "Sławek Biel"
  ],
  "messages": [
    {
      "sender": "Sławek Biel",
      "user_id": "U03E1LBTKV2",
      "message": "I created https://instabase.atlassian.net/browse/DR-2298 to track this.\n\nOne side effect of that is that we would be spending more on OpenAI calls in cases where we have multiple “Complex Prompts”. Today we cram them all into a single request, but if we want the same accuracy as in the chat we’d need to send them separately. Are we ok with that? CC @avi\nIf so I can make the code changes.\n\nAlso I assume we want it to match the gpt turbo chat mode, not the gpt-4 chat mode. Right?",
      "time": "05:22",
      "timestamp": "1689596564.534689",
      "is_reply": false
    },
    {
      "sender": "avi",
      "user_id": "U0477QPPM17",
      "message": "also cc: @Varun Jain for the cost implication of sending complex prompts separately.\n\nDo we have any benchmarking for how much worse the complex prompt performs when sent batched v/s separately? It will increase costs quite a bit and could mean that we need to charge more credits per complex prompt",
      "time": "05:25",
      "timestamp": "1689596732.363329",
      "is_reply": true
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Let us optimise for the performance first. @Sławek Biel @avi we should use the same pipeline as converse for complex prompts in Build. I’ll look into the pricing benchmarks again to make sure we’re fine on that front.\n\ncc: @Bastiane @Clemens",
      "time": "05:33",
      "timestamp": "1689597216.577719",
      "is_reply": true
    },
    {
      "sender": "avi",
      "user_id": "U0477QPPM17",
      "message": "Sounds good, let us know once we confirm this direction too. In case we are developing public apps, we will start to try harder with the simple prompts before trying complex prompts to try to keep them more cost effective. cc: @Shubham",
      "time": "05:35",
      "timestamp": "1689597315.155969",
      "is_reply": true
    },
    {
      "sender": "Varun Jain",
      "user_id": "U019KDMQL14",
      "message": "Again, let’s optimise for performance first, and do the cost optimisation second. As long asa you’re using gpt-3.5 turbo, we should be ok on costs overall.",
      "time": "05:39",
      "timestamp": "1689597566.130019",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2023-07-17.json",
    "message_count": 5,
    "start_time": "1689596564.534689",
    "end_time": "1689597566.130019",
    "is_thread": true
  }
}