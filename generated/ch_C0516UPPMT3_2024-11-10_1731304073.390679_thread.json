{
  "id": "ch_C0516UPPMT3_2024-11-10_1731304073.390679_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "avi"
  ],
  "messages": [
    {
      "sender": "avi",
      "user_id": "U0477QPPM17",
      "message": "*Announcing: Vision support available internally for Converse!* :eyes::frame_with_picture:  *cc: @Sławek Biel* *@lydia* \n\nEver wanted to get Converse to analyse an image or a chart-like graphic? Converse now uses OpenAI vision to be able to \"see\" visual information your documents. Try it out in  https://aihub-uat.internal.instabase.com/ by enabling `Enable vision reasoning field in Build projects` in Settings/Internal Preview Flags and then run a query using \"Research Mode\". This feature will soon be available in prod too with 24.44. Here is a documen (https://docs.google.com/document/d/1irEYBG-j9QF4V25A6RGV26P3rj9NcbrPAMDP4bpB55s/edit?usp=sharing)t with details on how it’s implemented.\n\nPlease try this with short, long, and multiple docs and share any feedback!",
      "time": "21:47",
      "timestamp": "1731304073.390679",
      "is_reply": false
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-11-10.json",
    "message_count": 1,
    "start_time": "1731304073.390679",
    "end_time": "1731304073.390679",
    "is_thread": true
  }
}