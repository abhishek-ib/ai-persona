{
  "id": "ch_C0516UPPMT3_2024-06-07_1717782866.504579_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Dale DeLoy",
    "Serena",
    "Sunny Khatri"
  ],
  "messages": [
    {
      "sender": "Dale DeLoy",
      "user_id": "U01T2LEP1FF",
      "message": "observation: Running single field extraction, first time running the prompt will see 3 out of 5 answers correct with 2 returning an empty set.  Second time running the exact same prompt getting 3 or 4 responses.  Third or fourth time running the prompt will see all 5 responses returning.  Any explanation for this behavior?  Seems like the model is learning from our submissions.",
      "time": "10:54",
      "timestamp": "1717782866.504579",
      "is_reply": false
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "what does 5 refer to? number of items in a list? number of documents for which we’re running the field?",
      "time": "11:13",
      "timestamp": "1717784023.563309",
      "is_reply": true
    },
    {
      "sender": "Dale DeLoy",
      "user_id": "U01T2LEP1FF",
      "message": "Number of documents running the field",
      "time": "11:14",
      "timestamp": "1717784063.648559",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "is this in refiner with ib_llm_tools?",
      "time": "11:15",
      "timestamp": "1717784114.076589",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "do you always get a response for all 5 documents after the 3rd/4th run? or do you sometimes get 3 or 4 out of 5 responses again?",
      "time": "11:16",
      "timestamp": "1717784164.999019",
      "is_reply": true
    },
    {
      "sender": "Dale DeLoy",
      "user_id": "U01T2LEP1FF",
      "message": "after the 3rd or 4th run consistently getting 5 responses",
      "time": "11:16",
      "timestamp": "1717784198.755159",
      "is_reply": true
    },
    {
      "sender": "Serena",
      "user_id": "U01CZ3LBFU4",
      "message": "cc @Sunny Khatri since you’ve been looking into Dale’s use case",
      "time": "11:17",
      "timestamp": "1717784259.022679",
      "is_reply": true
    },
    {
      "sender": "Sunny Khatri",
      "user_id": "U06KR4QB6E5",
      "message": "LLM behavior is non-deterministic and can have variations across multiple requests for same input.",
      "time": "11:21",
      "timestamp": "1717784495.958559",
      "is_reply": true
    },
    {
      "sender": "Dale DeLoy",
      "user_id": "U01T2LEP1FF",
      "message": "Our product is suppose to make the results as deterministic as possible.  Becoming more deterministic after more executions is what is being observed",
      "time": "11:23",
      "timestamp": "1717784606.240739",
      "is_reply": true
    },
    {
      "sender": "Sunny Khatri",
      "user_id": "U06KR4QB6E5",
      "message": "Yes. Unfortunately that's not how things work currently. Allowing users to just prompt anything meant we rely (or over rely) on LLMs to figure things out. Getting to a more stable and deterministic outcomes would need more investments from our side. This includes efforts in understanding user queries and response, which is big undertaking in itself. We're also not robust to LLM (OpenAI) changes, and the model there can also start producing different results.",
      "time": "11:34",
      "timestamp": "1717785268.358349",
      "is_reply": true
    },
    {
      "sender": "Sunny Khatri",
      "user_id": "U06KR4QB6E5",
      "message": "Our priorities are getting correctness right at this point and that seems to be the primary issue here. Something we are working towards.",
      "time": "12:18",
      "timestamp": "1717787933.359649",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-06-07.json",
    "message_count": 11,
    "start_time": "1717782866.504579",
    "end_time": "1717787933.359649",
    "is_thread": true
  }
}