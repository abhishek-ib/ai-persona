{
  "id": "ch_C0516UPPMT3_2024-07-30_1722377900.575719_conversation",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "conversation",
  "participants": [
    "Brian"
  ],
  "messages": [
    {
      "sender": "Brian",
      "user_id": "U06U3PWKM0T",
      "message": ":warning: Lengthy post incoming :warning:\n\n*OCR Confidence mechanism (and practical applications) question:*\n\nI see that the tooltips for `field extraction` `Field Confidence` and `OCR Confidence` read:\n> *Field confidence:*\n> *How confident the model is in the value extracted for this document.*\n> *OCR confidence:*\n> *How confident the OCR engine is for each word in this extracted field.*\nI feel like I understand the tooltip for `Field confidence` ; however, I feel it's more *difficult for me to understand the tooltip for OCR confidence as is.*\n\nUsing an example (screenshot1):\n\nMy guess without looking at the OCR confidence tooltip would be that the OCR confidence score represents how confident the OCR model was that the character or entity it recognized as a `1` is actually a `1` (for example.)\n\nIf we apply the OCR confidence score to a field value, the tooltip wording suggests to me that the OCR confidence represents how confident the model was that it's guess for the first word (ground truth: \"Tesla\") that is part of the field extraction value extracted from the page (top_1 predicted: \"Tesla\") is actually the word on the document (in this case, it is.)\n... since there are multiple words extracted from the document and part of the extracted value to assess, I assume the OCR Confidence score is some sort of weighted average of all of the scores for each word.\n\nIs this correct?\n\n*In terms of practical use-case:*\n• high OCR confidence: The characters recognized during the digitization stage have likely been captured accurately\n    ◦ Ex. Super machine readable text document\n• low OCR confidence: The characters recognized during the digitization stage may not have been captured accurately \n    ◦ Ex. Messy handwritten document with ambiguous characters\n• high field confidence: The value the product curated and gave to the user as a response is likely the thing you're asking for\n    ◦ Ex. \n        ▪︎ File: Clearly scanned driver license where all fields are labeled \n        ▪︎ field name: \"First Name\" \n        ▪︎ Model would likely be very confident that the name it returned to the user is what the user was looking for.\n• low field confidence: The value the product curated and gave to the user as a response is less likely to be thing you're asking for.\n    ◦ Ex. \n        ▪︎ File: Clearly scanned driver license where no fields are labeled and the license holder's last name is the same as their first name (John John -- silly example for demo purposes only :black_joker:) \n        ▪︎ field name: \"First Name\"\n        ▪︎ Model might be less confident that the name it returned to the user is what the user was looking for (less clear that the John is returned was actually the first name or the last name due to ambiguity in structure and values)\nIs this also correct?\n\nThank you! :bow:",
      "time": "15:18",
      "timestamp": "1722377900.575719",
      "is_reply": false
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2024-07-30.json",
    "message_count": 1,
    "start_time": "1722377900.575719",
    "end_time": "1722377900.575719",
    "is_thread": false
  }
}