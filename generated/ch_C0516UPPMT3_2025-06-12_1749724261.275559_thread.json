{
  "id": "ch_C0516UPPMT3_2025-06-12_1749724261.275559_thread",
  "type": "channel",
  "channel_name": "aihub-feedback",
  "conversation_type": "thread",
  "participants": [
    "Rosie",
    "Hamish"
  ],
  "messages": [
    {
      "sender": "Hamish",
      "user_id": "U06G3LGDBQU",
      "message": "Hey team, how long are the documents used in the Analyse limitation here? At this conference a lot of people are asking about how many files we can browse, it sounds like 500 is pretty good for most people but we are mainly talking about long docs e.g. 20-50 pages so would be good to understand this estimate better",
      "time": "03:31",
      "timestamp": "1749724261.275559",
      "is_reply": false
    },
    {
      "sender": "Rosie",
      "user_id": "U03EBULMVK3",
      "message": "@Balaram can answer this and any follow up questions better, but for now Iâ€™ll pass along the answer he gave me when I asked this exact question when documenting this change.  (Because I do know people want to know pages, but from what I understand with a token limit, a good faith estimate by page number is very hard because the pages could be very dense or mostly empty.)\n\n> We arrived at 500 file limit based on the front-end experience. With 20K chunks we were able to support till 3k to 5k documents (sparsely populated documents). So there is no easy way to map the chunk to the number of documents.\n> For 10K/Financial documents with dense text we will exhaust 20K chunks with ~150 documents.\n(To clarify, in this answer 20k chunks = the 4million token limit)",
      "time": "04:24",
      "timestamp": "1749727454.649709",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C0516UPPMT3",
    "channel_name": "aihub-feedback",
    "date_file": "2025-06-12.json",
    "message_count": 2,
    "start_time": "1749724261.275559",
    "end_time": "1749727454.649709",
    "is_thread": true
  }
}