{
  "id": "ch_C06FA6A23_2023-09-05_1693965920.299209_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "Anant",
    "Hari"
  ],
  "messages": [
    {
      "sender": "Anant",
      "user_id": "U0U100MNZ",
      "message": "posts on the internet suggest -- it runs pretty well -- https://mpost.io/llama-with-7-billion-parameters-achieves-lightning-fast-inference-on-apple-m2-max-chip/",
      "time": "19:05",
      "timestamp": "1693965920.299209",
      "is_reply": false
    },
    {
      "sender": "Hari",
      "user_id": "UCX3XL72Q",
      "message": "We didnâ€™t experiment mainly cos the inference times are very slow depending on the quantization and model param.. But we are keeping a close eye on this!\n\ncc: @vineeth",
      "time": "21:10",
      "timestamp": "1693973438.157459",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2023-09-05.json",
    "message_count": 2,
    "start_time": "1693965920.299209",
    "end_time": "1693973438.157459",
    "is_thread": true
  }
}