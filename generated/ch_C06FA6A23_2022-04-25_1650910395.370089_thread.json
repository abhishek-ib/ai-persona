{
  "id": "ch_C06FA6A23_2022-04-25_1650910395.370089_thread",
  "type": "channel",
  "channel_name": "discuss-engineering",
  "conversation_type": "thread",
  "participants": [
    "joshbronko",
    "Hari"
  ],
  "messages": [
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "Wanted to resurface this question to the group - https://instabase.slack.com/archives/C06FA6A23/p1650556055034599",
      "time": "11:13",
      "timestamp": "1650910395.370089",
      "is_reply": false
    },
    {
      "sender": "Hari",
      "user_id": "UCX3XL72Q",
      "message": "@joshbronko How are you planning to use this version? AFAIR with Spacy 2.x it was used only for simple consumptions like Sentencizer, Tekenization, NER etc. This all use to be consumed via custom UDFs.",
      "time": "11:16",
      "timestamp": "1650910584.379399",
      "is_reply": true
    },
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "Credit agreements where it can tokenize paragraphs and then find all the entities or orgs or dates. 3.x seems to function better than 2.x for some of these tasks",
      "time": "11:17",
      "timestamp": "1650910671.988929",
      "is_reply": true
    },
    {
      "sender": "Hari",
      "user_id": "UCX3XL72Q",
      "message": "And which model are you using? between en_core_web_sm, en_core_web_md and en_core_web_lg.. FYI on some history here, we had a lot of resourcing issues with spacy where these model were consuming good amount of memory in the already crowded app-task..",
      "time": "11:30",
      "timestamp": "1650911406.283989",
      "is_reply": true
    },
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "depends, md seems to perform the best / accuracy, havent tested the tranformers model as that seems to be a hog",
      "time": "11:30",
      "timestamp": "1650911452.702729",
      "is_reply": true
    },
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "Is there is another way to tokenize / sentencize it would be cool",
      "time": "11:31",
      "timestamp": "1650911471.254519",
      "is_reply": true
    },
    {
      "sender": "joshbronko",
      "user_id": "U031T0PNLUS",
      "message": "https://instabase.slack.com/archives/CJ7E9RG7Q/p1650907671030929",
      "time": "11:31",
      "timestamp": "1650911483.084709",
      "is_reply": true
    }
  ],
  "metadata": {
    "channel_id": "C06FA6A23",
    "channel_name": "discuss-engineering",
    "date_file": "2022-04-25.json",
    "message_count": 7,
    "start_time": "1650910395.370089",
    "end_time": "1650911483.084709",
    "is_thread": true
  }
}